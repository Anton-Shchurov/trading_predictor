{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ü–∞–π–ø–ª–∞–π–Ω –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –∏ —Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π. –õ–æ–≥–∏–∫–∞ —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —è—á–µ–π–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ—Ç–ª–∞–¥–∫–∏ –∏ –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (`X_train`, `folds`, `results` –∏ —Ç.–¥.).\n",
        "\n",
        "**–°—Ç—Ä—É–∫—Ç—É—Ä–∞:**\n",
        "1. –ò–º–ø–æ—Ä—Ç –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞\n",
        "2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "4. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Standard Training)\n",
        "5. –¢—é–Ω–∏–Ω–≥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Hyperparameter Tuning)\n",
        "6. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
        "\n",
        "–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—É—Ç–∏ –∫ `src` –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (`models.yml`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c646cbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import yaml\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π\n",
        "PROJECT_ROOT = Path(\"../..\").resolve()\n",
        "if str(PROJECT_ROOT / \"03_src\") not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT / \"03_src\"))\n",
        "\n",
        "# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è\n",
        "from models.modeling_pipeline import (\n",
        "    _load_yaml, _ensure_sorted_index, _validate_features, SplitConfig,\n",
        "    _temporal_train_valid_test_split, _time_series_folds, _compute_class_weights,\n",
        "    _fit_and_eval_model, _evaluate, _confusion_and_report, _predict_xgboost, _predict_catboost\n",
        ")\n",
        "from models.hyperparameter_tuning import tune_hyperparameters\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤\n",
        "MODELS_CONFIG_PATH = PROJECT_ROOT / \"04_configs/models.yml\"\n",
        "SPLITS_CONFIG_PATH = PROJECT_ROOT / \"04_configs/splits.yml\"\n",
        "\n",
        "with open(MODELS_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    models_config = yaml.safe_load(f)\n",
        "\n",
        "pipeline_settings = models_config.get(\"pipeline_settings\", {})\n",
        "RUN_TUNING = pipeline_settings.get(\"run_hyperparameter_tuning\", False)\n",
        "TARGET_COL = models_config.get(\"data\", {}).get(\"target_column\", \"y_buy_else_atr\")\n",
        "\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"Mode: {'HYPERPARAMETER TUNING' if RUN_TUNING else 'STANDARD TRAINING'}\")\n",
        "print(f\"Target Column: {TARGET_COL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d72700",
      "metadata": {},
      "source": [
        "## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–°—á–∏—Ç—ã–≤–∞–µ–º parquet-—Ñ–∞–π–ª, —É–∫–∞–∑–∞–Ω–Ω—ã–π –≤ –∫–æ–Ω—Ñ–∏–≥–µ, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤—ã–¥–µ–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ `X` –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521722a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "features_path = models_config.get(\"data\", {}).get(\"features_path\", \"01_data/processed/eurusd_features.parquet\")\n",
        "features_fp = PROJECT_ROOT / features_path\n",
        "\n",
        "if not features_fp.exists():\n",
        "    raise FileNotFoundError(f\"File not found: {features_fp}\")\n",
        "\n",
        "df = pd.read_parquet(features_fp)\n",
        "df = _ensure_sorted_index(df)\n",
        "X, y = _validate_features(df, target_col=TARGET_COL)\n",
        "\n",
        "print(f\"Dataset Shape: {X.shape}\")\n",
        "display(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c0624d",
      "metadata": {},
      "source": [
        "## 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (Cross-Validation)\n",
        "\n",
        "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞–∑–±–∏–µ–Ω–∏—è `TimeSeriesSplit` –Ω–∞ –æ—Å–Ω–æ–≤–µ `splits.yml`.  \n",
        "–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:\n",
        "- `X_train`, `X_valid`, `X_test`: –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏\n",
        "- `folds`: –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (Walk-Forward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a981c17",
      "metadata": {},
      "outputs": [],
      "source": [
        "splits_cfg_raw = _load_yaml(SPLITS_CONFIG_PATH)\n",
        "split_config = SplitConfig(\n",
        "    method=splits_cfg_raw.get(\"splits\", {}).get(\"method\", \"ratios\"),\n",
        "    ratios=splits_cfg_raw.get(\"splits\", {}).get(\"ratios\", {\"train\": 0.7, \"valid\": 0.15, \"test\": 0.15}),\n",
        "    dates=splits_cfg_raw.get(\"splits\", {}).get(\"dates\", {}),\n",
        "    n_splits=int(splits_cfg_raw.get(\"time_series_cv\", {}).get(\"n_splits\", 5)),\n",
        "    max_train_size=splits_cfg_raw.get(\"time_series_cv\", {}).get(\"max_train_size\"),\n",
        "    test_size=splits_cfg_raw.get(\"time_series_cv\", {}).get(\"test_size\"),\n",
        "    gap=int(splits_cfg_raw.get(\"time_series_cv\", {}).get(\"gap\", 0)),\n",
        "    artifacts=splits_cfg_raw.get(\"artifacts\", {}),\n",
        ")\n",
        "\n",
        "# 1. –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ (Train/Valid/Test)\n",
        "(X_train, y_train), (X_valid, y_valid), (X_test, y_test), split_meta = _temporal_train_valid_test_split(X, y, split_config)\n",
        "\n",
        "# 2. –§–æ–ª–¥—ã –¥–ª—è CV (–Ω–∞ Train + Valid)\n",
        "X_trval = pd.concat([X_train, X_valid])\n",
        "y_trval = pd.concat([y_train, y_valid])\n",
        "folds = _time_series_folds(X_trval, y_trval, split_config)\n",
        "\n",
        "print(\"Split Metadata:\", split_meta)\n",
        "print(f\"\\nNumber of CV Folds: {len(folds)}\")\n",
        "for i, (train_idx, valid_idx) in enumerate(folds):\n",
        "    print(f\"Fold {i+1}: Train size {len(train_idx)} | Valid size {len(valid_idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f77030",
      "metadata": {},
      "source": [
        "## 4. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è (Standard Training Loop)\n",
        "\n",
        "–≠—Ç–∞ —è—á–µ–π–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, **—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏** `run_hyperparameter_tuning: false`.\n",
        "- –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º enabled –º–æ–¥–µ–ª—è–º.\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (`use_best_params` -> `default` –∏–ª–∏ `best`).\n",
        "- –û–±—É—á–∞–µ—Ç –Ω–∞ —Ñ–æ–ª–¥–∞—Ö, —É—Å—Ä–µ–¥–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.\n",
        "- –§–∏–Ω–∞–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç –Ω–∞ Train+Valid –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–∞ Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f61e89",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "if not RUN_TUNING:\n",
        "    print(\"üöÄ Starting Standard Training...\\n\")\n",
        "    \n",
        "    # Import additional functions if needed\n",
        "    from models.modeling_pipeline import _find_best_threshold\n",
        "\n",
        "    # Calculate Class Weights\n",
        "    if pipeline_settings.get(\"use_class_weights\", False):\n",
        "        class_weights = _compute_class_weights(y_train)\n",
        "        print(f\"  -> Class Weights: 0={class_weights.get(0, 1.0):.2f}, 1={class_weights.get(1, 1.0):.2f}\")\n",
        "    else:\n",
        "        class_weights = {0: 1.0, 1: 1.0}\n",
        "        print(\"  -> Class Weights: Disabled (using 1.0 for all classes)\")\n",
        "\n",
        "    # Model Loop\n",
        "    enabled_models = [\n",
        "        name for name, cfg in (models_config.get(\"models\") or {}).items() \n",
        "        if (cfg or {}).get(\"enabled\", False)\n",
        "    ]\n",
        "    common_cfg = models_config.get(\"common\", {})\n",
        "    \n",
        "    for model_name in enabled_models:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing Model: {model_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        # Get model parameters\n",
        "        m_config = (models_config.get(\"models\", {}).get(model_name) or {})\n",
        "        use_best = m_config.get(\"use_best_params\", False)\n",
        "        base_params = m_config.get(\"default_params\", {})\n",
        "        best_p = m_config.get(\"best_params\", {})\n",
        "        \n",
        "        final_params = {**base_params, **best_p} if use_best else base_params\n",
        "        model_run_cfg = {\"params\": final_params, \"fit\": m_config.get(\"fit\", {})}\n",
        "        \n",
        "        print(f\"  -> Using {'best' if use_best else 'default'} params\")\n",
        "        \n",
        "        # --- Cross-Validation Loop ---\n",
        "        fold_metrics = []\n",
        "        fold_thresholds = []\n",
        "        \n",
        "        for fold_id, (tr_idx, va_idx) in enumerate(folds, start=1):\n",
        "            X_tr_f, y_tr_f = X_trval.iloc[tr_idx], y_trval.iloc[tr_idx]\n",
        "            X_va_f, y_va_f = X_trval.iloc[va_idx], y_trval.iloc[va_idx]\n",
        "            \n",
        "            # Fit model\n",
        "            clf, _ = _fit_and_eval_model(\n",
        "                model_name, X_tr_f, y_tr_f, X_va_f, y_va_f, class_weights, model_run_cfg, common_cfg\n",
        "            )\n",
        "            \n",
        "            # Get probabilities for threshold optimization\n",
        "            if model_name == \"catboost\":\n",
        "                _, proba_valid = _predict_catboost(clf, X_va_f)\n",
        "            elif model_name == \"xgboost\":\n",
        "                _, proba_valid = _predict_xgboost(clf, X_va_f)\n",
        "            else:  # lightgbm or other sklearn-compatible\n",
        "                proba_valid = clf.predict_proba(X_va_f)\n",
        "            \n",
        "            # FIX: Convert 2D proba array to 1D (class 1 probability) for ALL models\n",
        "            if isinstance(proba_valid, np.ndarray) and proba_valid.ndim == 2:\n",
        "                proba_valid = proba_valid[:, 1]\n",
        "            \n",
        "            # Find optimal threshold using validation set\n",
        "            best_thr, best_f1 = _find_best_threshold(y_va_f.values, proba_valid)\n",
        "            fold_thresholds.append(best_thr)\n",
        "            \n",
        "            # Re-evaluate with best threshold\n",
        "            pred_valid = (proba_valid >= best_thr).astype(int)\n",
        "            metrics_best = _evaluate(y_va_f.values, pred_valid, proba_valid)\n",
        "            metrics_best[\"selected_threshold\"] = best_thr\n",
        "            \n",
        "            fold_metrics.append(metrics_best)\n",
        "            print(f\"  -> Fold {fold_id}: Thr={best_thr:.2f} | F1={metrics_best.get('f1_class_1', 0):.4f} | PnL={metrics_best.get('simple_pnl', 0):+.1f}\")\n",
        "        \n",
        "        # Calculate average metrics across folds\n",
        "        avg_metrics = {\n",
        "            k: float(np.nanmean([m.get(k, np.nan) for m in fold_metrics])) \n",
        "            for k in fold_metrics[0].keys()\n",
        "        }\n",
        "        avg_threshold = float(np.mean(fold_thresholds))\n",
        "        avg_metrics[\"avg_threshold\"] = avg_threshold\n",
        "        \n",
        "        print(f\"  -> CV Avg: Thr={avg_threshold:.2f} | F1={avg_metrics.get('f1_class_1', 0):.4f} | PnL={avg_metrics.get('simple_pnl', 0):+.1f}\")\n",
        "\n",
        "        # --- Final Fit on Train+Valid & Test Evaluation ---\n",
        "        print(f\"\\n  -> Training final model on full Train+Valid set...\")\n",
        "        clf_final, _ = _fit_and_eval_model(\n",
        "            model_name, X_trval, y_trval, X_valid, y_valid, class_weights, model_run_cfg, common_cfg\n",
        "        )\n",
        "        \n",
        "        # Get test predictions\n",
        "        if model_name == \"catboost\":\n",
        "            _, y_proba_test = _predict_catboost(clf_final, X_test)\n",
        "        elif model_name == \"xgboost\":\n",
        "            _, y_proba_test = _predict_xgboost(clf_final, X_test)\n",
        "        else:\n",
        "            y_proba_test = clf_final.predict_proba(X_test)\n",
        "        \n",
        "        # FIX: Convert 2D proba array to 1D (class 1 probability)\n",
        "        if isinstance(y_proba_test, np.ndarray) and y_proba_test.ndim == 2:\n",
        "            y_proba_test_1 = y_proba_test[:, 1]\n",
        "        else:\n",
        "            y_proba_test_1 = y_proba_test\n",
        "        \n",
        "        # Apply average threshold from CV to test predictions\n",
        "        final_thr = avg_threshold  # Use avg from CV, not from avg_metrics dict\n",
        "        y_pred_test = (y_proba_test_1 >= final_thr).astype(int)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        test_metrics = _evaluate(y_test.values, y_pred_test, y_proba_test_1)\n",
        "        test_metrics[\"selected_threshold\"] = final_thr\n",
        "        test_details = _confusion_and_report(y_test.values, y_pred_test)\n",
        "        \n",
        "        print(f\"  -> Test: Thr={final_thr:.2f} | F1={test_metrics.get('f1_class_1', 0):.4f} | PnL={test_metrics.get('simple_pnl', 0):+.1f}\")\n",
        "\n",
        "        results[model_name] = {\n",
        "            \"cv_avg\": avg_metrics,\n",
        "            \"cv_folds\": fold_metrics,\n",
        "            \"test_metrics\": test_metrics,\n",
        "            \"test_details\": test_details,\n",
        "            \"model_obj\": clf_final\n",
        "        }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Standard Training Complete.\")\n",
        "    print(f\"   Models trained: {list(results.keys())}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Standard Training skipped (Mode = Hyperparameter Tuning)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf007560",
      "metadata": {},
      "source": [
        "## 5. –¢—é–Ω–∏–Ω–≥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Hyperparameter Tuning Loop)\n",
        "\n",
        "–≠—Ç–∞ —è—á–µ–π–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, **—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏** `run_hyperparameter_tuning: true`.\n",
        "- –ó–∞–ø—É—Å–∫–∞–µ—Ç Optuna/GridSearch.\n",
        "- –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n",
        "- –û–±–Ω–æ–≤–ª—è–µ—Ç `models.yml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfa9554",
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_TUNING:\n",
        "    print(\"üöÄ Starting Hyperparameter Tuning (Optuna)...\")\n",
        "    tuning_results = tune_hyperparameters(\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º models.yml –∫–∞–∫ –∫–æ–Ω—Ñ–∏–≥, —Ç.–∫. —Å–µ–∫—Ü–∏—è hp_tuning_params —Ç–∞–º\n",
        "        config_path=\"04_configs/models.yml\",\n",
        "        splits_config_path=str(SPLITS_CONFIG_PATH.relative_to(PROJECT_ROOT)),\n",
        "        models_config_path=str(MODELS_CONFIG_PATH.relative_to(PROJECT_ROOT)),\n",
        "        update_models_yaml=True\n",
        "    )\n",
        "    print(\"‚úÖ Tuning Complete. Best parameters saved to models.yml.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Tuning skipped (Mode = Standard Training)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7f07a7",
      "metadata": {},
      "source": [
        "## 6. –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ (Metrics Display)\n",
        "\n",
        "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5939d128",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame()\n",
        "\n",
        "if not RUN_TUNING and results:\n",
        "    data = []\n",
        "    for m, res in results.items():\n",
        "        row = {\"Model\": m}\n",
        "        row.update(res[\"test_metrics\"])\n",
        "        data.append(row)\n",
        "    \n",
        "    metrics_df = pd.DataFrame(data)\n",
        "    print(\"Test Set Metrics:\")\n",
        "    display(metrics_df)\n",
        "    \n",
        "elif RUN_TUNING and 'tuning_results' in locals():\n",
        "    print(\"Tuning Results Summary (Check artifacts directory for details).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b01fad6",
      "metadata": {},
      "source": [
        "## 7. –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç (Financial Report)\n",
        "\n",
        "–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ Simple PnL –∏ –ø–æ—Ä–æ–≥–∞–º –Ω–∞ Test Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271766f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not RUN_TUNING and results:\n",
        "    fin_data = []\n",
        "    for m, res in results.items():\n",
        "        metrics = res[\"test_metrics\"]\n",
        "        fin_data.append({\n",
        "            \"Model\": m,\n",
        "            \"Selected Threshold\": metrics.get(\"selected_threshold\"),\n",
        "            \"Simple PnL (R=1)\": metrics.get(\"simple_pnl\"),\n",
        "            \"F1 Class 1\": metrics.get(\"f1_class_1\"),\n",
        "            \"Precision\": metrics.get(\"precision_class_1\"),\n",
        "            \"Recall\": metrics.get(\"recall_class_1\")\n",
        "        })\n",
        "    \n",
        "    fin_df = pd.DataFrame(fin_data)\n",
        "    print(\"Financial Report (Test Set):\")\n",
        "    display(fin_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af31ca64",
      "metadata": {},
      "source": [
        "## 8. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (Visualization)\n",
        "\n",
        "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º (F1, Precision, Recall)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c1d8e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not RUN_TUNING and not metrics_df.empty:\n",
        "    target_cols = [\n",
        "        \"f1_class_1\", \"precision_class_1\", \"recall_class_1\", \"f1_class_0\", \n",
        "        \"f1_macro\", \"balanced_accuracy\", \"accuracy\", \"logloss\", \"roc_auc\"\n",
        "    ]\n",
        "    valid_cols = [c for c in target_cols if c in metrics_df.columns]\n",
        "    \n",
        "    if valid_cols:\n",
        "        ax = metrics_df.set_index(\"Model\")[valid_cols].plot(kind=\"bar\", figsize=(12, 7))\n",
        "        plt.title(\"Model Comparison (Test Set)\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        \n",
        "        # Add value labels\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f', padding=3, rotation=90)\n",
        "            \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8311d13a",
      "metadata": {},
      "source": [
        "## 9. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ (Confusion Matrix)\n",
        "\n",
        "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c663dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "if not RUN_TUNING and results:\n",
        "    for m, res in results.items():\n",
        "        cm = res[\"test_details\"][\"confusion_matrix\"]\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=[0, 1])\n",
        "        fig, ax = plt.subplots(figsize=(6, 5))\n",
        "        disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\")\n",
        "        ax.set_title(f\"Confusion Matrix: {m}\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116454f8",
      "metadata": {},
      "source": [
        "## 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n",
        "\n",
        "–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç (JSON/CSV), –º–æ–¥–µ–ª–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é `06_reports`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3041812",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import joblib\n",
        "import shutil\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, roc_curve\n",
        "\n",
        "if not RUN_TUNING and results:\n",
        "    # 1. Setup Directory Structure\n",
        "    exp_id = models_config.get(\"experiment\", {}).get(\"current_id\", \"exp_manual\")\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    save_dir = PROJECT_ROOT / \"06_reports\" / f\"{exp_id}_{timestamp}\"\n",
        "    \n",
        "    dirs = {\n",
        "        \"root\": save_dir,\n",
        "        \"models\": save_dir / \"models\",\n",
        "        \"plots\": save_dir / \"plots\",\n",
        "        \"metrics\": save_dir / \"metrics\"\n",
        "    }\n",
        "    for d in dirs.values():\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"üìÇ Saving experiment results to: {save_dir}\")\n",
        "\n",
        "    # 2. Save Config Snapshot\n",
        "    with open(dirs[\"metrics\"] / \"experiment_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(models_config, f, allow_unicode=True, sort_keys=False)\n",
        "\n",
        "    # 3. Save Metrics & Logs\n",
        "    if not metrics_df.empty:\n",
        "        metrics_path = dirs[\"metrics\"] / \"metrics_report.csv\"\n",
        "        metrics_df.to_csv(metrics_path, index=False)\n",
        "        print(f\"  -> Metrics saved: {metrics_path.name}\")\n",
        "        \n",
        "    results_serializable = {}\n",
        "    \n",
        "    # 4. Save Models & Plots per Model\n",
        "    for model_name, res in results.items():\n",
        "        # Save Model\n",
        "        model_subdir = dirs[\"models\"] / model_name\n",
        "        model_subdir.mkdir(exist_ok=True)\n",
        "        \n",
        "        clf = res.get(\"model_obj\")\n",
        "        if clf:\n",
        "            joblib.dump(clf, model_subdir / \"model.joblib\")\n",
        "            if model_name == \"catboost\":\n",
        "                try:\n",
        "                    clf.save_model(str(model_subdir / \"model.cbm\"))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        \n",
        "        # Save Plots (Regenerate to save file)\n",
        "        # Confusion Matrix\n",
        "        cm = res[\"test_details\"][\"confusion_matrix\"]\n",
        "        fig, ax = plt.subplots(figsize=(6, 5))\n",
        "        ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=[0, 1]).plot(ax=ax, cmap=\"Blues\")\n",
        "        ax.set_title(f\"Confusion Matrix: {model_name}\")\n",
        "        fig.savefig(dirs[\"plots\"] / f\"cm_{model_name}.png\", dpi=150)\n",
        "        plt.close(fig)\n",
        "        \n",
        "        # Prepare for JSON\n",
        "        results_serializable[model_name] = {k: v for k, v in res.items() if k != \"model_obj\"}\n",
        "\n",
        "    # 5. Save Model Comparison Plot\n",
        "    if not metrics_df.empty:\n",
        "        target_cols = [\n",
        "            \"f1_class_1\", \"precision_class_1\", \"recall_class_1\", \"f1_class_0\", \n",
        "            \"f1_macro\", \"balanced_accuracy\", \"accuracy\", \"logloss\", \"roc_auc\"\n",
        "        ]\n",
        "        valid_cols = [c for c in target_cols if c in metrics_df.columns]\n",
        "        \n",
        "        if valid_cols:\n",
        "            fig, ax = plt.subplots(figsize=(12, 7))\n",
        "            metrics_df.set_index(\"Model\")[valid_cols].plot(kind=\"bar\", ax=ax)\n",
        "            ax.set_title(f\"Model Comparison ({exp_id})\")\n",
        "            ax.set_ylabel(\"Score\")\n",
        "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            \n",
        "            # Add value labels\n",
        "            for container in ax.containers:\n",
        "                ax.bar_label(container, fmt='%.3f', padding=3, rotation=90)\n",
        "                \n",
        "            plt.tight_layout()\n",
        "            fig.savefig(dirs[\"plots\"] / \"models_comparison.png\", dpi=150)\n",
        "            plt.close(fig)\n",
        "\n",
        "    # Save Full JSON Report\n",
        "    with open(dirs[\"metrics\"] / \"metrics_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results_serializable, f, default=str, indent=2, ensure_ascii=False)\n",
        "        \n",
        "    print(\"‚úÖ All artifacts saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e5a812",
      "metadata": {},
      "source": [
        "## 11. –ê–≤—Ç–æ–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ CSV\n",
        "\n",
        "–ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ –æ–±—â–∏–π —Ñ–∞–π–ª `06_reports/experiments.csv` –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f8a923",
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–≤—Ç–æ–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ experiments.csv\n",
        "from models.experiment_logger import append_experiment_record\n",
        "\n",
        "if not RUN_TUNING and results:\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "    FE_CONFIG_PATH = PROJECT_ROOT / \"04_configs\" / \"feature_engineering.yml\"\n",
        "    with open(FE_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        fe_cfg = yaml.safe_load(f) or {}\n",
        "    \n",
        "    with open(SPLITS_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        splits_cfg = yaml.safe_load(f) or {}\n",
        "    \n",
        "    exp_id = models_config.get(\"experiment\", {}).get(\"current_id\", \"exp_manual\")\n",
        "    \n",
        "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ results –±–µ–∑ model_obj\n",
        "    results_for_log = {\n",
        "        name: {k: v for k, v in res.items() if k != \"model_obj\"}\n",
        "        for name, res in results.items()\n",
        "    }\n",
        "    \n",
        "    EXPERIMENTS_CSV = PROJECT_ROOT / \"06_reports\" / \"experiments.csv\"\n",
        "    \n",
        "    append_experiment_record(\n",
        "        experiments_csv_path=EXPERIMENTS_CSV,\n",
        "        experiment_id=str(exp_id).upper(),\n",
        "        results_dict=results_for_log,\n",
        "        metrics_dataframe=metrics_df,\n",
        "        splits_config=splits_cfg,\n",
        "        features_config=fe_cfg,\n",
        "        primary_metric=\"f1_class_1\",\n",
        "        seed=42,\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Experiment logging skipped (Mode = Hyperparameter Tuning or no results)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

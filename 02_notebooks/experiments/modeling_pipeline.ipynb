{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ ÐœÐ¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (Refactored)\n",
                "\n",
                "Ð­Ñ‚Ð¾Ñ‚ Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐº Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ Ð½Ð°Ð´ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ñ‚ÑŽÐ½Ð¸Ð½Ð³Ð° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð›Ð¾Ð³Ð¸ÐºÐ° Ñ€Ð°Ð·Ð±Ð¸Ñ‚Ð° Ð½Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÑ‡ÐµÐ¹ÐºÐ¸ Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð¸ Ð¸Ð½ÑÐ¿ÐµÐºÑ†Ð¸Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… (`X_train`, `folds`, `results` Ð¸ Ñ‚.Ð´.).\n",
                "\n",
                "**Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°:**\n",
                "1. Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°\n",
                "2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
                "3. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸\n",
                "4. Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Standard Training)\n",
                "5. Ð¢ÑŽÐ½Ð¸Ð½Ð³ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² (Hyperparameter Tuning)\n",
                "6. ÐÐ½Ð°Ð»Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ\n",
                "\n",
                "Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸, Ð½Ð°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð¿ÑƒÑ‚Ð¸ Ðº `src` Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ (`models.yml`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import yaml\n",
                "import joblib\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "\n",
                "# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÑƒÑ‚ÐµÐ¹\n",
                "PROJECT_ROOT = Path(\"../..\").resolve()\n",
                "if str(PROJECT_ROOT / \"03_src\") not in sys.path:\n",
                "    sys.path.append(str(PROJECT_ROOT / \"03_src\"))\n",
                "\n",
                "# Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð´Ð»Ñ Ð³Ñ€Ð°Ð½ÑƒÐ»ÑÑ€Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ\n",
                "from models.modeling_pipeline import (\n",
                "    _load_yaml, _ensure_sorted_index, _validate_features, SplitConfig,\n",
                "    _temporal_train_valid_test_split, _time_series_folds, _compute_class_weights,\n",
                "    _fit_and_eval_model, _evaluate, _confusion_and_report, _predict_xgboost, _predict_catboost\n",
                ")\n",
                "from models.hyperparameter_tuning import tune_hyperparameters\n",
                "\n",
                "# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð¾Ð²\n",
                "MODELS_CONFIG_PATH = PROJECT_ROOT / \"04_configs/models.yml\"\n",
                "SPLITS_CONFIG_PATH = PROJECT_ROOT / \"04_configs/splits.yml\"\n",
                "\n",
                "with open(MODELS_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "    models_config = yaml.safe_load(f)\n",
                "\n",
                "pipeline_settings = models_config.get(\"pipeline_settings\", {})\n",
                "RUN_TUNING = pipeline_settings.get(\"run_hyperparameter_tuning\", False)\n",
                "TARGET_COL = models_config.get(\"data\", {}).get(\"target_column\", \"y_buy_else_atr\")\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")\n",
                "print(f\"Mode: {'HYPERPARAMETER TUNING' if RUN_TUNING else 'STANDARD TRAINING'}\")\n",
                "print(f\"Target Column: {TARGET_COL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
                "\n",
                "Ð¡Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ parquet-Ñ„Ð°Ð¹Ð», ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ð² ÐºÐ¾Ð½Ñ„Ð¸Ð³Ðµ, ÑÐ¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ Ð²Ñ‹Ð´ÐµÐ»ÑÐµÐ¼ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² `X` Ð¸ Ñ†ÐµÐ»ÐµÐ²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ `y`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features_path = models_config.get(\"data\", {}).get(\"features_path\", \"01_data/processed/eurusd_features.parquet\")\n",
                "features_fp = PROJECT_ROOT / features_path\n",
                "\n",
                "if not features_fp.exists():\n",
                "    raise FileNotFoundError(f\"File not found: {features_fp}\")\n",
                "\n",
                "df = pd.read_parquet(features_fp)\n",
                "df = _ensure_sorted_index(df)\n",
                "X, y = _validate_features(df, target_col=TARGET_COL)\n",
                "\n",
                "print(f\"Dataset Shape: {X.shape}\")\n",
                "display(X.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ (Cross-Validation)\n",
                "\n",
                "Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ñ `TimeSeriesSplit` Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ `splits.yml`.  \n",
                "ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ:\n",
                "- `X_train`, `X_valid`, `X_test`: Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸\n",
                "- `folds`: Ð˜Ð½Ð´ÐµÐºÑÑ‹ Ð´Ð»Ñ ÐºÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ (Walk-Forward)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "splits_cfg_raw = _load_yaml(SPLITS_CONFIG_PATH)\n",
                "split_config = SplitConfig(\n",
                "    method=splits_cfg_raw.get(\"splits\", {}).get(\"method\", \"ratios\"),\n",
                "    ratios=splits_cfg_raw.get(\"splits\", {}).get(\"ratios\", {\"train\": 0.7, \"valid\": 0.15, \"test\": 0.15}),\n",
                "    dates=splits_cfg_raw.get(\"splits\", {}).get(\"dates\", {}),\n",
                "    n_splits=int(splits_cfg_raw.get(\"time_series_cv\", {}).get(\"n_splits\", 5)),\n",
                "    max_train_size=splits_cfg_raw.get(\"time_series_cv\", {}).get(\"max_train_size\"),\n",
                "    test_size=splits_cfg_raw.get(\"time_series_cv\", {}).get(\"test_size\"),\n",
                "    gap=int(splits_cfg_raw.get(\"time_series_cv\", {}).get(\"gap\", 0)),\n",
                "    artifacts=splits_cfg_raw.get(\"artifacts\", {}),\n",
                ")\n",
                "\n",
                "# 1. Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ (Train/Valid/Test)\n",
                "(X_train, y_train), (X_valid, y_valid), (X_test, y_test), split_meta = _temporal_train_valid_test_split(X, y, split_config)\n",
                "\n",
                "# 2. Ð¤Ð¾Ð»Ð´Ñ‹ Ð´Ð»Ñ CV (Ð½Ð° Train + Valid)\n",
                "X_trval = pd.concat([X_train, X_valid])\n",
                "y_trval = pd.concat([y_train, y_valid])\n",
                "folds = _time_series_folds(X_trval, y_trval, split_config)\n",
                "\n",
                "print(\"Split Metadata:\", split_meta)\n",
                "print(f\"\\nNumber of CV Folds: {len(folds)}\")\n",
                "for i, (train_idx, valid_idx) in enumerate(folds):\n",
                "    print(f\"Fold {i+1}: Train size {len(train_idx)} | Valid size {len(valid_idx)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (Standard Training Loop)\n",
                "\n",
                "Ð­Ñ‚Ð° ÑÑ‡ÐµÐ¹ÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ, **Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸** `run_hyperparameter_tuning: false`.\n",
                "- ÐŸÑ€Ð¾Ñ…Ð¾Ð´Ð¸Ñ‚ Ð¿Ð¾ Ð²ÑÐµÐ¼ enabled Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼.\n",
                "- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (`use_best_params` -> `default` Ð¸Ð»Ð¸ `best`).\n",
                "- ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ Ð½Ð° Ñ„Ð¾Ð»Ð´Ð°Ñ…, ÑƒÑÑ€ÐµÐ´Ð½ÑÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸.\n",
                "- Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ Ð½Ð° Train+Valid Ð¸ Ñ‚ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð½Ð° Test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = {}\n",
                "\n",
                "if not RUN_TUNING:\n",
                "    print(\"ðŸš€ Starting Standard Training...\\n\")\n",
                "    \n",
                "    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð²ÐµÑÐ° ÐºÐ»Ð°ÑÑÐ¾Ð², ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾\n",
                "    if pipeline_settings.get(\"use_class_weights\", False):\n",
                "        class_weights = _compute_class_weights(y_train)\n",
                "        print(f\"Class Weights: {class_weights}\")\n",
                "    else:\n",
                "        class_weights = {0: 1.0, 1: 1.0}\n",
                "\n",
                "    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n",
                "    enabled_models = [name for name, cfg in (models_config.get(\"models\") or {}).items() if (cfg or {}).get(\"enabled\", False)]\n",
                "    common_cfg = models_config.get(\"common\", {})\n",
                "    \n",
                "    for model_name in enabled_models:\n",
                "        print(f\"\\nProcessing Model: {model_name}...\")\n",
                "        \n",
                "        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²\n",
                "        m_config = (models_config.get(\"models\", {}).get(model_name) or {})\n",
                "        use_best = m_config.get(\"use_best_params\", False)\n",
                "        base_params = m_config.get(\"default_params\", {})\n",
                "        best_p = m_config.get(\"best_params\", {})\n",
                "        \n",
                "        final_params = {**base_params, **best_p} if use_best else base_params\n",
                "        model_run_cfg = {\"params\": final_params, \"fit\": m_config.get(\"fit\", {})}\n",
                "        \n",
                "        print(f\"  -> Use Best Params: {use_best}\")\n",
                "        \n",
                "        # --- Cross-Validation Loop ---\n",
                "        fold_metrics = []\n",
                "        for fold_id, (tr_idx, va_idx) in enumerate(folds, start=1):\n",
                "            X_tr_f, y_tr_f = X_trval.iloc[tr_idx], y_trval.iloc[tr_idx]\n",
                "            X_va_f, y_va_f = X_trval.iloc[va_idx], y_trval.iloc[va_idx]\n",
                "            \n",
                "            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑŽÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
                "            clf, metrics = _fit_and_eval_model(\n",
                "                model_name, X_tr_f, y_tr_f, X_va_f, y_va_f, class_weights, model_run_cfg, common_cfg\n",
                "            )\n",
                "            fold_metrics.append(metrics)\n",
                "            \n",
                "        avg_metrics = {k: float(np.nanmean([m.get(k, np.nan) for m in fold_metrics])) for k in fold_metrics[0].keys()}\n",
                "        print(f\"  -> CV Average F1 (Class 1): {avg_metrics.get('f1_class_1', 0):.4f}\")\n",
                "\n",
                "        # --- Final Fit & Test ---\n",
                "        clf_final, _ = _fit_and_eval_model(\n",
                "            model_name, X_trval, y_trval, X_valid, y_valid, class_weights, model_run_cfg, common_cfg\n",
                "        )\n",
                "        \n",
                "        if model_name == \"catboost\":\n",
                "            y_pred_test, y_proba_test = _predict_catboost(clf_final, X_test)\n",
                "        elif model_name == \"xgboost\":\n",
                "            y_pred_test, y_proba_test = _predict_xgboost(clf_final, X_test)\n",
                "        else:\n",
                "            y_proba_test = clf_final.predict_proba(X_test)\n",
                "            y_pred_test = clf_final.predict(X_test)\n",
                "            \n",
                "        test_metrics = _evaluate(y_test.values, y_pred_test, y_proba_test)\n",
                "        test_details = _confusion_and_report(y_test.values, y_pred_test)\n",
                "\n",
                "        results[model_name] = {\n",
                "            \"cv_avg\": avg_metrics,\n",
                "            \"test_metrics\": test_metrics,\n",
                "            \"test_details\": test_details,\n",
                "            \"model_obj\": clf_final\n",
                "        }\n",
                "\n",
                "    print(\"\\nâœ… Standard Training Complete.\")\n",
                "else:\n",
                "    print(\"âš ï¸ Standard Training skipped (Mode = Hyperparameter Tuning)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Ð¢ÑŽÐ½Ð¸Ð½Ð³ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² (Hyperparameter Tuning Loop)\n",
                "\n",
                "Ð­Ñ‚Ð° ÑÑ‡ÐµÐ¹ÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ, **Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸** `run_hyperparameter_tuning: true`.\n",
                "- Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Optuna/GridSearch.\n",
                "- ÐÐ°Ñ…Ð¾Ð´Ð¸Ñ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹.\n",
                "- ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ `models.yml`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_TUNING:\n",
                "    print(\"ðŸš€ Starting Hyperparameter Tuning (Optuna)...\")\n",
                "    tuning_results = tune_hyperparameters(\n",
                "        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ models.yml ÐºÐ°Ðº ÐºÐ¾Ð½Ñ„Ð¸Ð³, Ñ‚.Ðº. ÑÐµÐºÑ†Ð¸Ñ hp_tuning_params Ñ‚Ð°Ð¼\n",
                "        config_path=\"04_configs/models.yml\",\n",
                "        splits_config_path=str(SPLITS_CONFIG_PATH.relative_to(PROJECT_ROOT)),\n",
                "        models_config_path=str(MODELS_CONFIG_PATH.relative_to(PROJECT_ROOT)),\n",
                "        update_models_yaml=True\n",
                "    )\n",
                "    print(\"âœ… Tuning Complete. Best parameters saved to models.yml.\")\n",
                "else:\n",
                "    print(\"âš ï¸ Tuning skipped (Mode = Standard Training)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Ð’Ñ‹Ð²Ð¾Ð´ Ð¼ÐµÑ‚Ñ€Ð¸Ðº (Metrics Display)\n",
                "\n",
                "Ð¡Ñ€Ð°Ð²Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð²ÑÐµÑ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics_df = pd.DataFrame()\n",
                "\n",
                "if not RUN_TUNING and results:\n",
                "    data = []\n",
                "    for m, res in results.items():\n",
                "        row = {\"Model\": m}\n",
                "        row.update(res[\"test_metrics\"])\n",
                "        data.append(row)\n",
                "    \n",
                "    metrics_df = pd.DataFrame(data)\n",
                "    print(\"Test Set Metrics:\")\n",
                "    display(metrics_df)\n",
                "    \n",
                "elif RUN_TUNING and 'tuning_results' in locals():\n",
                "    print(\"Tuning Results Summary (Check artifacts directory for details).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ (Visualization)\n",
                "\n",
                "Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ð¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼ (F1, Precision, Recall)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not RUN_TUNING and not metrics_df.empty:\n",
                "    cols = [\"f1_class_1\", \"precision_class_1\", \"recall_class_1\", \"roc_auc\"]\n",
                "    valid_cols = [c for c in cols if c in metrics_df.columns]\n",
                "    \n",
                "    metrics_df.set_index(\"Model\")[valid_cols].plot(kind=\"bar\", figsize=(10, 6))\n",
                "    plt.title(\"Model Comparison (Test Set)\")\n",
                "    plt.ylabel(\"Score\")\n",
                "    plt.xticks(rotation=0)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. ÐÐ½Ð°Ð»Ð¸Ð· Ð¾ÑˆÐ¸Ð±Ð¾Ðº (Confusion Matrix)\n",
                "\n",
                "Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ† Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð´Ð»Ñ Ð±Ð¾Ð»ÐµÐµ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import ConfusionMatrixDisplay\n",
                "\n",
                "if not RUN_TUNING and results:\n",
                "    for m, res in results.items():\n",
                "        cm = res[\"test_details\"][\"confusion_matrix\"]\n",
                "        disp = ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=[0, 1])\n",
                "        fig, ax = plt.subplots(figsize=(6, 5))\n",
                "        disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\")\n",
                "        ax.set_title(f\"Confusion Matrix: {m}\")\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð²\n",
                "\n",
                "Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚ (JSON/CSV), Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸ Ð² Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ `06_reports`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "import json\n",
                "import joblib\n",
                "import shutil\n",
                "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, roc_curve\n",
                "\n",
                "if not RUN_TUNING and results:\n",
                "    # 1. Setup Directory Structure\n",
                "    exp_id = models_config.get(\"experiment\", {}).get(\"current_id\", \"exp_manual\")\n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    save_dir = PROJECT_ROOT / \"06_reports\" / f\"{exp_id}_{timestamp}\"\n",
                "    \n",
                "    dirs = {\n",
                "        \"root\": save_dir,\n",
                "        \"models\": save_dir / \"models\",\n",
                "        \"plots\": save_dir / \"plots\",\n",
                "        \"metrics\": save_dir / \"metrics\"\n",
                "    }\n",
                "    for d in dirs.values():\n",
                "        d.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    print(f\"ðŸ“‚ Saving experiment results to: {save_dir}\")\n",
                "\n",
                "    # 2. Save Config Snapshot\n",
                "    with open(dirs[\"metrics\"] / \"experiment_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
                "        yaml.safe_dump(models_config, f, allow_unicode=True, sort_keys=False)\n",
                "\n",
                "    # 3. Save Metrics & Logs\n",
                "    if not metrics_df.empty:\n",
                "        metrics_path = dirs[\"metrics\"] / \"metrics_report.csv\"\n",
                "        metrics_df.to_csv(metrics_path, index=False)\n",
                "        print(f\"  -> Metrics saved: {metrics_path.name}\")\n",
                "        \n",
                "    results_serializable = {}\n",
                "    \n",
                "    # 4. Save Models & Plots per Model\n",
                "    for model_name, res in results.items():\n",
                "        # Save Model\n",
                "        model_subdir = dirs[\"models\"] / model_name\n",
                "        model_subdir.mkdir(exist_ok=True)\n",
                "        \n",
                "        clf = res.get(\"model_obj\")\n",
                "        if clf:\n",
                "            joblib.dump(clf, model_subdir / \"model.joblib\")\n",
                "            if model_name == \"catboost\":\n",
                "                try:\n",
                "                    clf.save_model(str(model_subdir / \"model.cbm\"))\n",
                "                except Exception:\n",
                "                    pass\n",
                "        \n",
                "        # Save Plots (Regenerate to save file)\n",
                "        # Confusion Matrix\n",
                "        cm = res[\"test_details\"][\"confusion_matrix\"]\n",
                "        fig, ax = plt.subplots(figsize=(6, 5))\n",
                "        ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=[0, 1]).plot(ax=ax, cmap=\"Blues\")\n",
                "        ax.set_title(f\"Confusion Matrix: {model_name}\")\n",
                "        fig.savefig(dirs[\"plots\"] / f\"cm_{model_name}.png\", dpi=150)\n",
                "        plt.close(fig)\n",
                "        \n",
                "        # Prepare for JSON\n",
                "        results_serializable[model_name] = {k: v for k, v in res.items() if k != \"model_obj\"}\n",
                "\n",
                "    # 5. Save Model Comparison Plot\n",
                "    if not metrics_df.empty:\n",
                "        cols = [\"f1_class_1\", \"precision_class_1\", \"recall_class_1\", \"roc_auc\"]\n",
                "        valid_cols = [c for c in cols if c in metrics_df.columns]\n",
                "        \n",
                "        fig, ax = plt.subplots(figsize=(10, 6))\n",
                "        metrics_df.set_index(\"Model\")[valid_cols].plot(kind=\"bar\", ax=ax)\n",
                "        ax.set_title(f\"Model Comparison ({exp_id})\")\n",
                "        ax.set_ylabel(\"Score\")\n",
                "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "        plt.tight_layout()\n",
                "        fig.savefig(dirs[\"plots\"] / \"models_comparison.png\", dpi=150)\n",
                "        plt.close(fig)\n",
                "\n",
                "    # Save Full JSON Report\n",
                "    with open(dirs[\"metrics\"] / \"metrics_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "        json.dump(results_serializable, f, default=str, indent=2, ensure_ascii=False)\n",
                "        \n",
                "    print(\"âœ… All artifacts saved successfully.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

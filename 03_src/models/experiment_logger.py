"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ experiments.csv.

–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ YAML –∫–æ–Ω—Ñ–∏–≥–æ–≤ –∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É.
"""

from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd


def _round3(x: Optional[float]) -> Optional[float]:
    """–û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 3 –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π None –∏ NaN."""
    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):
        return None
    return float(np.round(x, 3))


def _infer_asset_tf(fe_cfg: dict) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ asset/timeframe –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ feature engineering."""
    try:
        ds = fe_cfg.get("dataset", {})
        active_key = ds.get("active", "")
        item = ds.get("items", {}).get(active_key, {})
        symbol = item.get("symbol")
        tf = item.get("timeframe")
        if symbol and tf:
            return f"{str(symbol).upper()}/{str(tf).upper()}"
    except Exception:
        pass
    # Fallback: –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑ dataset.active (eurusd_h1_...) -> EURUSD/H1
    try:
        active_key = fe_cfg.get("dataset", {}).get("active", "")
        parts = str(active_key).split("_")
        if len(parts) >= 2:
            return f"{parts[0].upper()}/{parts[1].upper()}"
    except Exception:
        pass
    return "UNKNOWN/TF"


def _infer_ids(fe_cfg: dict) -> Tuple[str, str, str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ dataset_id, target_id, fset_id –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞."""
    dataset_id = str(fe_cfg.get("dataset", {}).get("active", "unknown_dataset"))
    
    # Target ID –∏–∑ feature_definitions
    fd = fe_cfg.get("feature_definitions", {})
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–±–æ–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y_def = fd.get("y_buy_else_atr", fd.get("y_bs", {}))
    params = y_def.get("params", {})
    horizon = params.get("horizon", "?")
    method = y_def.get("method", "binary_target")
    h_part = f"h{int(horizon)}" if isinstance(horizon, (int, float)) else "h?"
    m_part = "binary" if "binary" in str(method).lower() else str(method).lower()
    target_id = f"target_{h_part}_{m_part}"
    
    # Feature set
    fset_id = str(fe_cfg.get("pipeline_settings", {}).get("feature_set", "fset-?"))
    
    return dataset_id, target_id, fset_id


def append_experiment_record(
    experiments_csv_path: Path,
    experiment_id: str,
    results_dict: Dict,
    metrics_dataframe: pd.DataFrame,
    splits_config: dict,
    features_config: dict,
    primary_metric: str = "f1_class_1",
    seed: int = 42,
    model_name: Optional[str] = None,
    params_str: Optional[str] = None,
) -> None:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø–∏—Å—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ experiments.csv.
    
    Args:
        experiments_csv_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É experiments.csv
        experiment_id: ID —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "EXP_0020")
        results_dict: –°–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –º–æ–¥–µ–ª—è–º (–±–µ–∑ model_obj)
        metrics_dataframe: DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–∫–æ–ª–æ–Ω–∫–∞ "Model" + –º–µ—Ç—Ä–∏–∫–∏)
        splits_config: –ö–æ–Ω—Ñ–∏–≥ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (splits.yml)
        features_config: –ö–æ–Ω—Ñ–∏–≥ —Ñ–∏—á (feature_engineering.yml)
        primary_metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        seed: Random seed
        model_name: –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∞—è)
        params_str: –°—Ç—Ä–æ–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø–∏—Å–∏
    """
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ primary_metric
    df = metrics_dataframe.copy()
    if primary_metric in df.columns:
        df = df.sort_values(by=primary_metric, ascending=False)
    chosen_model = model_name or (df["Model"].iloc[0] if not df.empty else None)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    metrics_map: Dict[str, Optional[float]] = {}
    up_pct, down_pct = None, None
    
    if chosen_model and chosen_model in results_dict:
        test_metrics = results_dict[chosen_model].get("test_metrics", {})
        cv_avg = results_dict[chosen_model].get("cv_avg", {})
        
        metrics_map = {
            "accuracy": test_metrics.get("accuracy", cv_avg.get("accuracy")),
            "f1_class_1": test_metrics.get("f1_class_1", cv_avg.get("f1_class_1")),
            "precision_class_1": test_metrics.get("precision_class_1", cv_avg.get("precision_class_1")),
            "recall_class_1": test_metrics.get("recall_class_1", cv_avg.get("recall_class_1")),
            "balanced_accuracy": test_metrics.get("balanced_accuracy", cv_avg.get("balanced_accuracy")),
            "roc_auc": test_metrics.get("roc_auc", cv_avg.get("roc_auc")),
            "simple_pnl": test_metrics.get("simple_pnl", cv_avg.get("simple_pnl")),
            "selected_threshold": test_metrics.get("selected_threshold", cv_avg.get("avg_threshold")),
        }
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ confusion matrix
        details = results_dict[chosen_model].get("test_details", {})
        cm = np.array(details.get("confusion_matrix", []))
        if cm.size > 0:
            supports = cm.sum(axis=1)
            total = float(supports.sum()) if supports.sum() else 1.0
            down_pct = float(supports[0]) / total
            up_pct = float(supports[1]) / total
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
    asset_tf = _infer_asset_tf(features_config)
    dataset_id, target_id, fset_id = _infer_ids(features_config)
    
    # –°—Ö–µ–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    cv_cfg = splits_config.get("time_series_cv", {})
    n_splits = cv_cfg.get("n_splits")
    gap = cv_cfg.get("gap")
    validation_str = f"tscv_k={n_splits}" if n_splits else "tscv"
    if gap:
        validation_str += f"_gap={gap}"
    
    # –ö–æ–ª–æ–Ω–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
    columns = [
        "ID", "Date", "Asset/TF", "Dataset", "Target", "Feature set", "Model", "Params",
        "Validation", "Seed", "Acc", "F1", "Precision", "Recall", "BalancedAcc",
        "ROC-AUC", "SimplePnL", "Threshold", "Up %", "Down %", "Primary metric", "Primary value",
    ]
    
    now_utc = datetime.now(timezone.utc).isoformat(timespec="seconds")
    
    row = [
        experiment_id,
        now_utc,
        asset_tf,
        dataset_id,
        target_id,
        fset_id,
        chosen_model or "",
        params_str or "",
        validation_str,
        seed,
        _round3(metrics_map.get("accuracy")),
        _round3(metrics_map.get("f1_class_1")),
        _round3(metrics_map.get("precision_class_1")),
        _round3(metrics_map.get("recall_class_1")),
        _round3(metrics_map.get("balanced_accuracy")),
        _round3(metrics_map.get("roc_auc")),
        _round3(metrics_map.get("simple_pnl")),
        _round3(metrics_map.get("selected_threshold")),
        _round3(up_pct),
        _round3(down_pct),
        primary_metric,
        _round3(metrics_map.get(primary_metric)),
    ]
    
    # –ó–∞–ø–∏—Å—å –≤ CSV
    experiments_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    if experiments_csv_path.exists():
        df_csv = pd.read_csv(experiments_csv_path)
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        for col in columns:
            if col not in df_csv.columns:
                df_csv[col] = None
        df_csv = df_csv.reindex(columns=columns)
        df_csv = pd.concat([df_csv, pd.DataFrame([row], columns=columns)], ignore_index=True)
    else:
        df_csv = pd.DataFrame([row], columns=columns)
    
    df_csv.to_csv(experiments_csv_path, index=False)
    print(f"üìù Experiment record added to: {experiments_csv_path}")
